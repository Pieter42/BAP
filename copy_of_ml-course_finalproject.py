# -*- coding: utf-8 -*-
"""Copy of Assignment_week8_FinalProjectA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p9R68jCC9tVhP-YKYabXg6OSqP3tmF-3

# Final Project A: Pulsar Star Classification

In this final project, you are expected to create __two classifiers__ for classification of Pulsar stars from non-palsar stars. This is a binary classification problem, however note that this is an imbalanced classification problem as only about 10% of the data is actually pulsar stars (see introduction and data section for more details). You are free to decide the classification algorithms and the structure of your work but make sure you cover the following points in this project.

1. Developing two classifiers for your data: One classifier of your choice from linear learning and one from non-linear learning algorithms (neural networks or random forests). Play around with hyper-parameters and choose those that provides the best results. 
2. Show the results (balanced accuracy and confusion matrices) for both classifiers and both test and training data, discuss how your classifiers work in terms of bias and variance.
3. For one of the classifiers, provide a measure of feature importances and briefly analyze the importances of features.
4. Use functions from open source libraries like sci-kit learn and keras and avoid using your own hand-written functions from previous assignments.  

Feel free to contact us in Teams if you need more description.

This notebook is structured like a scientific paper. The text should provide a high-level overview of your approach. Please don't include any details about your code in the text but add them as comments in the code itself. Your code should be clean and readable with enough comments.

As before let's start with cloning [the course's Github](https://github.com/bahareh368/introduction_to_ML) to import the datasets.
"""

import sys
import os

if not os.path.exists(os.getcwd()+'/introduction_to_ML'):
    !git clone https://github.com/bahareh368/introduction_to_ML.git
else:
    print('"/introduction_to_ML" already exists. If you want to update this folder for:\
    Colab: factory reset your runtime and run this cell again.\
    Jupytor Notebook: Delete the folder in your current directory and run this cell again.')
    # append the folder to sys.path and import costume modeules

sys.path.append("introduction_to_ML")
import intro2ML_functions as i2ml

"""## 1. Introduction
Neuron stars appear when stars run out of fuel and collapse. This creates a large explosion in space, also known as a supernova. Pulsars are a special type of such Neuron stars. Pulsars namely produce a certain type of radio emmision, which is detectable on earth. Because of this reason, scientists are interested in analyzing Pulsars. However, performing analyses on stars is costly and time-consuming. This means that it is of vital importance that candidate Pulsar stars are detected before the analysis itself. Doing this by hand, though, is not trivial. (You can find more information on pulsar stars [here](https://www.space.com/32661-pulsars.html) and [here](https://imagine.gsfc.nasa.gov/science/objects/neutron_stars1.html) )

Therefore, this study proposes a machine learning-based classification algorithm to classify between Pulsar stars and non-Pulsar stars. 
This study is organized as follows: in Section 2 we introduce the dataset, preprocessing techniques, and classification. In Section 3 we present our results and finally in Section 4 we evaluate our approach and conclude the study.

## 2. Methods

### 2.1 Data
The dataset (from [here](https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification#)) contains pulsar candidates collected during the High Time Resolution Universe Survey. The data contains 1,639 examples of actual pulsars, the remaining 16,259 examples are false examples, caused by noise.

The dataset contains 8 features:

1. Mean of the integrated profile.
2. Standard deviation of the integrated profile.
3. Excess kurtosis of the integrated profile.
4. Skewness of the integrated profile.
5. Mean of the DM-SNR curve.
6. Standard deviation of the DM-SNR curve.
7. Excess kurtosis of the DM-SNR curve.
8. Skewness of the DM-SNR curve.
"""

#@title Import Libraries { form-width: "38%" }

import numpy as np
import matplotlib.pyplot as plt
import sklearn as sk
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Perceptron
from sklearn.metrics import confusion_matrix
from sklearn.metrics import balanced_accuracy_score
from sklearn.neural_network import MLPClassifier
import sklearn.preprocessing
import sklearn.impute

#@title Load data { form-width: "38%" }
# from numpy import genfromtxt
data = np.genfromtxt('introduction_to_ML/pulsar_data_train.csv', dtype=float, delimiter=',', skip_header=1)

#@title Split in train and test data { form-width: "38%" }

X = data[:,:-1]
y = data[:,-1]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### 2.2. Explore the data
<font color=#6698FF> It is recommended that you explore the data before you start coding. Think about making some scatter plots, histograms or printing the data, but feel free to choose any method that suits you.  </font>

<font color=#ff0000>To get familiar with the data, a few scatterplots were made. Three of these are shown below. They depict different features of different data points. In the last plot, a lot of data points are visible on the x-axis, these indicate missing data, which are stored as a NaN but shown as a zero. 

Also some data is shown in matrix form, to get acquainted with the order of magnitude of some features. Once again, NaNs are found.</font>
"""

#@title Plotting some data points { form-width: "38%" }

def plot_2class(x_in,y_in):
    #Create a figure
    plt.figure()
    colors = ['tab:blue', 'tab:orange', 'tab:green']
    #iterate over the classes
    for i in range(2):
        #select only the points with class i and plot them in the right colors
        mask = (y_in.astype(int)==i)
        plt.scatter(x_in[0,mask[0,:]], x_in[1,mask[0,:]], marker = 'o', color = colors[i], label = 'y = {}'.format(i))
    #finish the plot
    plt.legend()
    plt.axis('scaled')
    plt.xlabel('PC1'); plt.ylabel('PC2');

plot_x = X_train[0:1000,0:2].T
plot_y = (y_train.T)[np.newaxis,0:1000]

plot_2class(plot_x,plot_y)

plot_x = X_train[500:700,3:5].T
plot_y = (y_train.T)[np.newaxis,500:700]

plot_2class(plot_x,plot_y)

plot_x = X_train[1000:3000,6:8].T
plot_y = (y_train.T)[np.newaxis,1000:3000]

plot_2class(plot_x,plot_y)

#@title Printing some data points { form-width: "38%" }

print(X_train[1000:1010,3:6])   # 3 features for 10 samples
print(y_train[1000:1010])       # classes of the above samples

"""### 2.3. Preprocessing
<font color=#6698FF> Here, some preprocessing techniques can be applied if needed, mainly standard normalization of the data and checking for missing values and a way for handling them. Note that you are only allowed to use training data for preprocessing but you then need to perform similar changes on test data too.</font>

<font color=#ff0000>Before using the data, it has to be processed for optimal predictions. First, the data should be nonrmalized. 

While exploring the data, it was noticed that the data set consists of NaNs. These have to be detected and handled (by substituting it by the median of the feature). The previously shown data points are printed once again to ensure that the NaN is removed.</font>
"""

#@title Standardization { form-width: "38%" }

scaler = sk.preprocessing.StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

#@title Checking and handling missing values { form-width: "38%" }

imp = sk.impute.SimpleImputer(missing_values=np.nan, strategy='median')

X_train = imp.fit_transform(X_train)
X_test = imp.fit_transform(X_test)

print(X_train[1000:1010,3:6])

"""### 2.6. Classification

<font color=#6698FF> Briefly introduced your two classification algorithms and provide the codes for them both in seperate code cells. Note that training and test data have already been seperated for you. 
</font>

<font color=#ff0000>Two models are used to classify the data: a linear model, being the perceptron, and a non-linear model, being the Multi-layer Perceptron classifier.</font>
"""

#@title Linear Model { form-width: "38%" }

clf_lin = Perceptron(tol=1e-3, random_state=0)
clf_lin.fit(X_train, y_train)

#@title Non-Linear Model { form-width: "38%" }

clf_nonlin = MLPClassifier(solver='lbfgs', max_iter=2000, alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)
clf_nonlin.fit(X_train, y_train)

"""
## 3. Results
<font color=#6698FF> Present your final confusion matrices (2 by 2) and balanced accuracies for both test and training data and both classifiers. Analyse the performance on test and training in terms of bias and variance.

You should also provide a measure of feature importance here for one of your classifiers.</font>

<font color=#ff0000>Both the Linear and Non-Linear models have unbalanced and balanced accuracies above 90%. The balanced accuracy is, however, clearly lower. Nonetheless, a balanced accuracy of 91% is acceptable.

It can be noticed from the confusion matrix that there is a bias for the model to predict a sample as a non-Pulsar.</font>"""

#@title Linear Model { form-width: "38%" }

y_pred = clf_lin.predict(X_test)
print('Confusion Matrix for Linear Model:\n',confusion_matrix(y_test, y_pred),'\n')

print('The balanced score for the Linear Model is:', balanced_accuracy_score(y_test, y_pred))
print('The unbalanced score for the Linear Model is: ', clf_lin.score(X_test, y_test))

#@title Non-Linear Model { form-width: "38%" }

y_pred = clf_nonlin.predict(X_test)
print('Confusion Matrix for Linear Model:\n',confusion_matrix(y_test, y_pred),'\n')

print('The balanced score for the Non-Linear Model is:', balanced_accuracy_score(y_test, y_pred))
print('The unbalanced score for the Non-Linear Model is: ', clf_nonlin.score(X_test, y_test))

"""## 4. Discussion and Conclusion

<font color=#6698FF> Discuss all the choices you made during the process and your final conclusions.  Highlight the strong points of your approach, discuss its shortcomings  and suggest some future approaches that may improve it.</font>

<font color=#ff0000>A perceptron and neural network algorithm were used to classify data consisting of Pulsars and non-Pulsars. The accuracy was very satisfactory with a balanced score of 91% for both algorithms, with the neural network slightly outperforming the perceptron. 

A bias towards non-Pulsars was noticed, however. This might be solved by retrieving better features or more data points. Also using a more complex classification method might yield better results as different classification methods get better results for different data sets.</font>

## 5. Comments on the course intro2ml

The course structure, lectures and assignments were all awesome. We very much enjoyed (casually) learning a lot about Machine Learning on the side. The balance between theory and practice was perfect.

We also very much appreciated how quickly questions on MS teams were answered, it made a big difference for our workflow.

Thank you, Bahareh and Obin!
"""